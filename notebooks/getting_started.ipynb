{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c994ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rsharma228/miniconda/762/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/rsharma228/miniconda/762/lib/python3.13/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd84be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9d62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/2] Loading HumanEval (code reasoning)...\n",
      "\n",
      "[2/2] Loading GSM8K (math reasoning)...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/2] Loading HumanEval (code reasoning)...\")\n",
    "code_dataset = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "\n",
    "print(\"\\n[2/2] Loading GSM8K (math reasoning)...\")\n",
    "math_dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b359a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Example CODE prompt:\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any t...\n",
      "\n",
      "============================================================\n",
      "Example MATH prompt:\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the rem...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 20\n",
    "code_prompts = [code_dataset[i][\"prompt\"] for i in range(min(N_SAMPLES, len(code_dataset)))]\n",
    "math_prompts = [math_dataset[i][\"question\"] for i in range(min(N_SAMPLES, len(math_dataset)))]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example CODE prompt:\")\n",
    "print(code_prompts[0][:150] + \"...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example MATH prompt:\")\n",
    "print(math_prompts[0][:150] + \"...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34315939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Qwen2.5-0.5B-Instruct first (smallest reasoning model)\n",
    "# Fallback to Phi-2 if needed\n",
    "MODEL_OPTIONS = [\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",  # 500MB, great reasoning\n",
    "    \"microsoft/phi-2\",              # 2.7GB, excellent for math/code\n",
    "    \"gpt2\",                         # 500MB, fallback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d2be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 22.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "  Name: microsoft/phi-2\n",
      "  Layers: 32\n",
      "  Hidden size: 2560\n",
      "  Parameters: ~2780M\n"
     ]
    }
   ],
   "source": [
    "model_name = MODEL_OPTIONS[1]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "    n_layers = len(model.model.layers)\n",
    "    hidden_size = model.config.hidden_size\n",
    "elif hasattr(model, 'transformer'):\n",
    "    n_layers = len(model.transformer.h)\n",
    "    hidden_size = model.config.n_embd if hasattr(model.config, 'n_embd') else model.config.hidden_size\n",
    "else:\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    hidden_size = model.config.hidden_size\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Name: {model_name}\")\n",
    "print(f\"  Layers: {n_layers}\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.0f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_activations(prompts, model, tokenizer, max_length=256, batch_size=4):\n",
    "    \"\"\"\n",
    "    Collect activations from all layers.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of text prompts\n",
    "        model: The language model\n",
    "        tokenizer: Tokenizer\n",
    "        max_length: Maximum sequence length\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping layer indices to activation arrays\n",
    "    \"\"\"\n",
    "    activations = {i: [] for i in range(n_layers)}\n",
    "    \n",
    "    print(f\"Collecting activations from {n_layers} layers...\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch_prompts = prompts[batch_start:batch_start + batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        # Extract activations\n",
    "        hidden_states = outputs.hidden_states\n",
    "        \n",
    "        # Get sequence lengths (last non-padding token)\n",
    "        seq_lengths = inputs['attention_mask'].sum(dim=1) - 1\n",
    "        \n",
    "        for layer_idx in range(hidden_states.shape[0]):\n",
    "            # hidden_states[0] is embedding, hidden_states[1] is layer 0, etc.\n",
    "            layer_output = hidden_states[layer_idx + 1]\n",
    "            \n",
    "            # Get last token activation for each sequence in batch\n",
    "            for batch_idx in range(layer_output.shape[0]):\n",
    "                last_token_pos = seq_lengths[batch_idx]\n",
    "                last_token_activation = layer_output[batch_idx, last_token_pos, :].cpu().numpy()\n",
    "                activations[layer_idx].append(last_token_activation)\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    for layer_idx in activations:\n",
    "        activations[layer_idx] = np.stack(activations[layer_idx])\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73507581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Collecting CODE activations...\n",
      "======================================================================\n",
      "Collecting activations from 32 layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 21.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations from 32 layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 21.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collected code activations\n",
      "  Shape per layer: torch.Size([4, 168, 51200])\n",
      "✓ Collected math activations\n",
      "  Shape per layer: torch.Size([4, 61, 51200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Collecting CODE activations...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "code_activations = collect_activations(\n",
    "    code_prompts,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_length=256,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "math_activations = collect_activations(\n",
    "    math_prompts,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_length=256,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"✓ Collected code activations\")\n",
    "print(f\"  Shape per layer: {code_activations[0].shape}\")\n",
    "\n",
    "print(f\"✓ Collected math activations\")\n",
    "print(f\"  Shape per layer: {math_activations[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1c0976b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 168, 51200])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_activations.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de10e458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 61, 51200])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_activations.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76ce5f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 168, 2560])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_activations.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2de4cd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 61, 2560])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_activations.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11ddaf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02519 ,  0.1805  ,  0.2485  , ..., -0.01395 , -0.0681  ,\n",
       "         0.1164  ],\n",
       "       [-0.0792  , -0.09314 ,  0.405   , ...,  0.095   , -0.1986  ,\n",
       "        -0.181   ],\n",
       "       [-0.0823  , -0.004875,  0.2698  , ...,  0.02847 ,  0.02025 ,\n",
       "         0.0785  ],\n",
       "       ...,\n",
       "       [-0.013016,  0.321   ,  0.05008 , ..., -0.03964 , -0.09143 ,\n",
       "        -0.2032  ],\n",
       "       [-0.1818  ,  0.1556  ,  0.6733  , ...,  0.12134 ,  0.0448  ,\n",
       "        -0.00711 ],\n",
       "       [-0.005325,  0.10626 , -0.05026 , ..., -0.05283 , -0.1613  ,\n",
       "        -0.1427  ]], shape=(20, 2560), dtype=float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_activations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3649772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting activations from 32 layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:00, 17.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 127, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 124, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n",
      "torch.Size([4, 177, 2560])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:00<00:00, 17.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n",
      "torch.Size([4, 114, 2560])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n",
      "torch.Size([4, 168, 2560])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "activations = {i: [] for i in range(n_layers)}\n",
    "\n",
    "prompts = code_prompts\n",
    "max_length=256\n",
    "batch_size=4\n",
    "print(f\"Collecting activations from {n_layers} layers...\")\n",
    "\n",
    "# Process in batches\n",
    "for batch_start in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[batch_start:batch_start + batch_size]\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        batch_prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Extract activations\n",
    "    hidden_states = outputs.hidden_states\n",
    "    \n",
    "    # Get sequence lengths (last non-padding token)\n",
    "    seq_lengths = inputs['attention_mask'].sum(dim=1) - 1\n",
    "    \n",
    "    for layer_idx in range(len(hidden_states)-1):\n",
    "        # hidden_states[0] is embedding, hidden_states[1] is layer 0, etc.\n",
    "        layer_output = hidden_states[layer_idx + 1]\n",
    "        print(layer_output.shape)\n",
    "        \n",
    "        # Get last token activation for each sequence in batch\n",
    "        for batch_idx in range(layer_output.shape[0]):\n",
    "            last_token_pos = seq_lengths[batch_idx]\n",
    "            last_token_activation = layer_output[batch_idx, last_token_pos, :].cpu().numpy()\n",
    "            activations[layer_idx].append(last_token_activation)\n",
    "\n",
    "# Convert lists to arrays\n",
    "for layer_idx in activations:\n",
    "    activations[layer_idx] = np.stack(activations[layer_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c582a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
